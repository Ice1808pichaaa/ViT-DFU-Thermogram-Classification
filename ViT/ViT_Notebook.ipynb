{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NOc2Kxys-k1"
   },
   "source": [
    "# **Setup & Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mnZqGrLthLiP",
    "outputId": "0318c749-305b-494a-e3bb-217baffeff58"
   },
   "outputs": [],
   "source": [
    "!pip install -U \"transformers>=4.44\"\n",
    "!pip install -U \"accelerate>=0.33\"\n",
    "!pip install -U \"datasets>=2.20\"\n",
    "!pip install -U \"evaluate>=0.4\"\n",
    "!pip install \"huggingface-hub>=0.34.0,<1.0\"\n",
    "!pip install -U scikit-learn\n",
    "!pip install -U sacrebleu\n",
    "!pip install -U gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSK4NFtfHMfm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import itertools\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from transformers import (\n",
    "    ViTForImageClassification,\n",
    "    ViTImageProcessor,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    "    pipeline,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08IgchiktFty"
   },
   "source": [
    "# **Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AESL0T_SOzJH",
    "outputId": "b9696356-d999-4b73-bb88-30d188997124"
   },
   "outputs": [],
   "source": [
    "!gdown 1fr-ZuEeJGi5kE1pA3EzlTvHfr-oN5U0P\n",
    "!unzip Dataset_DFUs.zip\n",
    "!rm Dataset_DFUs.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MT410tuqOzJH",
    "outputId": "aa3644ce-72d5-4d1b-a521-a263ccb182b1"
   },
   "outputs": [],
   "source": [
    "count = glob.glob('Dataset/**/*.png', recursive=True)\n",
    "print(\"Dataset amount:\", len(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ihDMWjr4HMfp",
    "outputId": "a03df9d8-869a-4cf0-e44c-25fb656e805e"
   },
   "outputs": [],
   "source": [
    "img_name = []\n",
    "img_label = []\n",
    "\n",
    "for file in glob.glob('Dataset/**/*.png', recursive=True):\n",
    "    img_name.append(file)\n",
    "    img_label.append(int(file.split(\"/\")[-2]))\n",
    "\n",
    "df_master = pd.DataFrame({\n",
    "    'img_path': img_name,\n",
    "    'label': img_label\n",
    "})\n",
    "\n",
    "print(\"Total images loaded:\")\n",
    "print(df_master['label'].value_counts())\n",
    "\n",
    "X = df_master['img_path']\n",
    "y = df_master['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "2bnNj7n6urNa",
    "outputId": "94d32d04-1015-4386-e9bb-f2f69aeee079"
   },
   "outputs": [],
   "source": [
    "df_master.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRBw-L0EtWfp"
   },
   "source": [
    "# **Configuration & Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xfh12JISD95W"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = 'google/vit-base-patch16-224'\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(model_checkpoint)\n",
    "\n",
    "id2label = { 0: '0', 1: '1' }\n",
    "label2id = { '0': 0, '1': 1 }\n",
    "\n",
    "def transform(example_batch):\n",
    "    inputs = feature_extractor([Image.open(x) for x in example_batch['img_path']], return_tensors='pt')\n",
    "    inputs['labels'] = example_batch['label']\n",
    "    return inputs\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"labels\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "acc_metric  = evaluate.load(\"accuracy\")\n",
    "prec_metric = evaluate.load(\"precision\")\n",
    "rec_metric  = evaluate.load(\"recall\")\n",
    "f1_metric   = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\":  acc_metric.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"precision\": prec_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"precision\"],\n",
    "        \"recall\":    rec_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"recall\"],\n",
    "        \"f1\":        f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOs1XD3tteF3"
   },
   "source": [
    "# **Cross-Validation Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZptheodoOzJI",
    "outputId": "fb61126a-922d-49e0-ff39-c40756a33c2d"
   },
   "outputs": [],
   "source": [
    "class_weights_array = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y),\n",
    "    y=y\n",
    ")\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights_array, dtype=torch.float32)\n",
    "\n",
    "print(f\"Using class weights: {class_weights_tensor}\")\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = class_weights.to(self.args.device)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        if self.class_weights is not None and labels is not None:\n",
    "            loss_fct = CrossEntropyLoss(weight=self.class_weights)\n",
    "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        else:\n",
    "            loss = outputs.loss if outputs.loss is not None else super().compute_loss(model, inputs, return_outputs)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ZSo-HX-S2xh"
   },
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "all_metrics = []\n",
    "all_true_labels = np.array([], dtype=int)\n",
    "all_predictions = np.array([], dtype=int)\n",
    "fold_logs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tRR-p_74S4fS",
    "outputId": "747f97b7-fa69-43d8-c44b-d9e5416e213c"
   },
   "outputs": [],
   "source": [
    "print(\"--- Visualizing 5-Fold Split Distributions ---\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"\\n--- FOLD {fold + 1}/{N_SPLITS} ---\")\n",
    "\n",
    "    val_df = df_master.iloc[val_idx]\n",
    "    val_labels = y.iloc[val_idx]\n",
    "\n",
    "    val_counts = val_labels.value_counts().sort_index()\n",
    "    print(f\"Validation set distribution:\\n{val_counts}\\n\")\n",
    "\n",
    "    print(f\"Displaying 5 sample validation images for Fold {fold + 1}:\")\n",
    "\n",
    "    sample_df = val_df.sample(min(5, len(val_df)))\n",
    "    image_paths = sample_df['img_path'].tolist()\n",
    "    image_labels = sample_df['label'].tolist()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 5, figsize=(15, 3))\n",
    "\n",
    "    for i, (path, label) in enumerate(zip(image_paths, image_labels)):\n",
    "        try:\n",
    "            img = Image.open(path)\n",
    "            axs[i].imshow(img)\n",
    "            axs[i].set_title(f\"Label: {label}\")\n",
    "            axs[i].axis('off')\n",
    "        except FileNotFoundError:\n",
    "            axs[i].set_title(\"Image Not Found\")\n",
    "            axs[i].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wdmni1ylxe4Z",
    "outputId": "d30c04b2-0690-4bb2-ceaa-07fe59f1c8cc"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class_weights_tensor = class_weights_tensor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iyi9WsjBtqsX"
   },
   "source": [
    "**Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "55ed875a4eea4a3ba302fd11d707e085",
      "a506aaff122f416ea37e65cd6747080c",
      "e223fff9ee7a4cf3b66d7f05d7625707",
      "36902fcce263413389a8559c41027b40",
      "8ff2ebadccdd41d79259acafa7a8cdc5",
      "be480f806c51486fbbd1d9db7e3f47d0",
      "6c70beeff51f45969a0174a9935104fb",
      "10ea7835464947e8ac27ceb5e2e39f39",
      "c6fe2d79ba0b40359e8db00264e822de",
      "70e3af9dc6b84a3ea55c491602aa96b7",
      "74cbc351aca54a51a7738c8cfbe582b0",
      "0387642f1b474a01abb94969dac1c12d",
      "4c57a08a7827411a948965927d0f9e58",
      "005ceda9230f401aa85178cd3e10d988",
      "fa084b4ae1434651883d1d51860baa49",
      "5b4ad87b0f71477a8dbc2492e5f718d4",
      "841ff244b96d4d0cbeadf89338057b56",
      "cc099bce4d5346fd98257af1170c19a9",
      "c2b58e23b18e48ed939e6910a12a4648",
      "0496c889229f4a36b064737b56ff8a0a",
      "1a4b77d7cd7344a5b520bed6b38a0af3",
      "924981d1d5464a7fb3ac8b056cff4a13"
     ]
    },
    "id": "huYPmRM4OzJI",
    "outputId": "4174be0e-cd8c-4ef4-e0a1-7cc5f395f003"
   },
   "outputs": [],
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"\\n--- FOLD {fold + 1}/{N_SPLITS} ---\")\n",
    "\n",
    "    set_seed(42 + fold)\n",
    "\n",
    "    train_df = df_master.iloc[train_idx]\n",
    "    val_df   = df_master.iloc[val_idx]\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(train_df).with_transform(transform)\n",
    "    val_dataset   = Dataset.from_pandas(val_df).with_transform(transform)\n",
    "\n",
    "    model = ViTForImageClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        num_labels=2,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"training_ViT_fold_{fold+1}\",\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=16,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=15,\n",
    "        logging_steps=10,\n",
    "        learning_rate=3e-5,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.05,\n",
    "        save_total_limit=2,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        remove_unused_columns=False,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        load_best_model_at_end=True,\n",
    "        use_mps_device= torch.backends.mps.is_available(),\n",
    "        optim=\"adamw_torch\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=collate_fn,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=feature_extractor,\n",
    "        class_weights=class_weights_tensor,\n",
    "        # callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    best_dir = trainer.state.best_model_checkpoint or training_args.output_dir\n",
    "\n",
    "    export_dir = f\"export_vit_fold_{fold+1}\"\n",
    "\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "    shutil.copytree(best_dir, export_dir, dirs_exist_ok=True)\n",
    "\n",
    "    feature_extractor.save_pretrained(export_dir)\n",
    "\n",
    "    torch.save({\"id2label\": id2label, \"label2id\": label2id}, os.path.join(export_dir, \"label_map.pt\"))\n",
    "\n",
    "    print(f\"[Fold {fold+1}] Saved best model to: {export_dir}\")\n",
    "\n",
    "    print(f\"[Fold {fold+1}] Best metric (F1):\", trainer.state.best_metric)\n",
    "    print(f\"[Fold {fold+1}] Best checkpoint:\", trainer.state.best_model_checkpoint)\n",
    "    print(f\"[Fold {fold+1}] Epochs actually trained:\", trainer.state.epoch)\n",
    "\n",
    "    fold_logs.append(trainer.state.log_history)\n",
    "\n",
    "    log_file_path = os.path.join(export_dir, f\"fold_{fold+1}_log_history.json\")\n",
    "    with open(log_file_path, 'w') as f:\n",
    "        json.dump(trainer.state.log_history, f, indent=4)\n",
    "    print(f\"[Fold {fold+1}] Saved log history to: {log_file_path}\")\n",
    "\n",
    "    print(f\"--- Evaluating Fold {fold+1} ---\")\n",
    "    metrics = trainer.evaluate()\n",
    "    print(metrics)\n",
    "\n",
    "    predictions_output = trainer.predict(val_dataset)\n",
    "    fold_preds = np.argmax(predictions_output.predictions, axis=1)\n",
    "\n",
    "    all_predictions = np.concatenate((all_predictions, fold_preds))\n",
    "    all_true_labels = np.concatenate((all_true_labels, predictions_output.label_ids))\n",
    "    all_probs_pos   = np.array([], dtype=float)\n",
    "\n",
    "    logits = predictions_output.predictions\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    probs = exp_logits / exp_logits.sum(axis=1, keepdims=True)\n",
    "    probs_pos = probs[:, 1]\n",
    "\n",
    "    # keep collecting\n",
    "    all_probs_pos = np.concatenate((all_probs_pos, probs_pos))\n",
    "\n",
    "\n",
    "    del model, trainer, predictions_output\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "print(\"\\n--- Cross-Validation Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-XsdgX4tkZO"
   },
   "source": [
    "# **Results & Analysis for Cross Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tg_d61nLDhSn"
   },
   "source": [
    "## **Helper function to plot the graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83HovxKhz32p"
   },
   "outputs": [],
   "source": [
    "def curves_from_history(log_history):\n",
    "    df = pd.DataFrame(log_history)\n",
    "\n",
    "    if \"loss\" in df.columns:\n",
    "        train_curve = (\n",
    "            df[df[\"loss\"].notna()]\n",
    "            .groupby(\"epoch\", as_index=False)[\"loss\"]\n",
    "            .last()\n",
    "            .rename(columns={\"loss\": \"train_loss\"})\n",
    "        )\n",
    "    else:\n",
    "        train_curve = pd.DataFrame(columns=[\"epoch\", \"train_loss\"])\n",
    "\n",
    "    if \"eval_loss\" in df.columns:\n",
    "        agg_dict = {\n",
    "            \"eval_loss\": (\"eval_loss\", \"last\")\n",
    "        }\n",
    "        if \"eval_accuracy\" in df.columns:\n",
    "            agg_dict[\"eval_accuracy\"] = (\"eval_accuracy\", \"last\")\n",
    "        if \"eval_f1\" in df.columns:\n",
    "            agg_dict[\"eval_f1\"] = (\"eval_f1\", \"last\")\n",
    "\n",
    "        eval_curve = (\n",
    "            df[df[\"eval_loss\"].notna()]\n",
    "            .groupby(\"epoch\", as_index=False)\n",
    "            .agg(**agg_dict)\n",
    "        )\n",
    "    else:\n",
    "        eval_curve = pd.DataFrame(columns=[\"epoch\", \"eval_loss\", \"eval_accuracy\", \"eval_f1\"])\n",
    "\n",
    "    curve = pd.merge(train_curve, eval_curve, on=\"epoch\", how=\"outer\").sort_values(\"epoch\")\n",
    "    return curve\n",
    "\n",
    "def smooth_xy(x, y, points=200):\n",
    "    m = ~np.isnan(x) & ~np.isnan(y)\n",
    "    x, y = np.asarray(x)[m], np.asarray(y)[m]\n",
    "\n",
    "    order = np.argsort(x)\n",
    "    x, y = x[order], y[order]\n",
    "\n",
    "    uniq_x, uniq_idx = np.unique(x, return_index=True)\n",
    "    uniq_y = y[uniq_idx]\n",
    "\n",
    "    if len(uniq_x) < 4:\n",
    "        x_dense = np.linspace(uniq_x.min(), uniq_x.max(), points)\n",
    "        y_dense = np.interp(x_dense, uniq_x, uniq_y)\n",
    "        return x_dense, y_dense\n",
    "\n",
    "    spline = make_interp_spline(uniq_x, uniq_y, k=3)\n",
    "\n",
    "    x_dense = np.linspace(uniq_x.min(), uniq_x.max(), points)\n",
    "    y_dense = spline(x_dense)\n",
    "\n",
    "    return x_dense, y_dense\n",
    "\n",
    "fold_curves = [curves_from_history(h) for h in fold_logs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxpgOMh6DuY9"
   },
   "source": [
    "## Plot the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ma3i2Kkuz_Oy",
    "outputId": "3224f628-82fb-4ea8-e57b-310cfdee709e"
   },
   "outputs": [],
   "source": [
    "for i, curve in enumerate(fold_curves, 1):\n",
    "    plt.figure()\n",
    "\n",
    "    x_acc, y_acc = smooth_xy(curve[\"epoch\"].values, curve[\"eval_accuracy\"].values)\n",
    "    plt.plot(x_acc, y_acc, label=\"Eval accuracy\", marker='')\n",
    "\n",
    "    x_f1, y_f1 = smooth_xy(curve[\"epoch\"].values, curve[\"eval_f1\"].values)\n",
    "    plt.plot(x_f1, y_f1, label=\"Eval F1\", marker='')\n",
    "\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(f\"Fold {i} — Accuracy & F1 per epoch\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6Ude5PVe0DFL",
    "outputId": "31c62cff-c6c8-4fcd-c6e8-7ae6f6c5b9b7"
   },
   "outputs": [],
   "source": [
    "for i, curve in enumerate(fold_curves, 1):\n",
    "    plt.figure()\n",
    "\n",
    "    x_tr, y_tr = smooth_xy(curve[\"epoch\"].values, curve[\"train_loss\"].values)\n",
    "    plt.plot(x_tr, y_tr, label=\"Train loss\", marker='')\n",
    "\n",
    "    x_ev, y_ev = smooth_xy(curve[\"epoch\"].values, curve[\"eval_loss\"].values)\n",
    "    plt.plot(x_ev, y_ev, label=\"Eval loss\", marker='')\n",
    "\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Fold {i} — Loss per epoch\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "id": "-nvxJAZ9BhRX",
    "outputId": "9a48ecc2-6c1e-48f9-c949-f5ae97c8dc91"
   },
   "outputs": [],
   "source": [
    "all_eval = []\n",
    "all_train = []\n",
    "for c in fold_curves:\n",
    "    all_eval.append(c[[\"epoch\", \"eval_loss\", \"eval_accuracy\", \"eval_f1\"]].dropna())\n",
    "    all_train.append(c[[\"epoch\", \"train_loss\"]].dropna())\n",
    "\n",
    "avg_eval = (pd.concat(all_eval)\n",
    "            .groupby(\"epoch\", as_index=False)\n",
    "            .mean(numeric_only=True))\n",
    "avg_train = (pd.concat(all_train)\n",
    "             .groupby(\"epoch\", as_index=False)\n",
    "             .mean(numeric_only=True))\n",
    "\n",
    "plt.figure()\n",
    "x_train_loss, y_train_loss = smooth_xy(avg_train[\"epoch\"], avg_train[\"train_loss\"])\n",
    "plt.plot(x_train_loss, y_train_loss, label=\"Avg train loss\", marker='')\n",
    "x_eval_loss, y_eval_loss = smooth_xy(avg_eval[\"epoch\"],  avg_eval[\"eval_loss\"])\n",
    "plt.plot(x_eval_loss, y_eval_loss, label=\"Avg eval loss\", marker='')\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Average Loss across folds\")\n",
    "plt.legend(); plt.grid(True, alpha=0.3); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "x_eval_acc, y_eval_acc = smooth_xy(avg_eval[\"epoch\"], avg_eval[\"eval_accuracy\"])\n",
    "plt.plot(x_eval_acc, y_eval_acc, label=\"Avg eval accuracy\", marker='', color='orange')\n",
    "\n",
    "x_eval_f1, y_eval_f1 = smooth_xy(avg_eval[\"epoch\"], avg_eval[\"eval_f1\"])\n",
    "plt.plot(x_eval_f1, y_eval_f1, label=\"Avg eval F1\", marker='')\n",
    "\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Score\")\n",
    "plt.title(\"Average Eval Accuracy & F1 Score across folds\")\n",
    "plt.legend(); plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YadHyZLzxzS"
   },
   "source": [
    "**Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "bzn7uuDdfLf5",
    "outputId": "2d2b908d-ee31-4405-beba-9da447be08b1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(all_true_labels, all_predictions)\n",
    "labels = [\"Class 0\", \"Class 1\"] \n",
    "\n",
    "plt.figure(figsize=(4.5, 4))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    cbar=True,\n",
    "    xticklabels=labels,\n",
    "    yticklabels=labels,\n",
    "    linewidths=0.5,\n",
    "    linecolor=\"white\"\n",
    ")\n",
    "plt.title(\"Overall Confusion Matrix (all folds)\", pad=12)\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9s48gl1RUUB7"
   },
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "j83JbKj0dTma",
    "outputId": "ef176e3a-dbd4-4760-fa31-1ccf1da095ee"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import ViTForImageClassification, Trainer, TrainingArguments\n",
    "\n",
    "all_metrics      = []   \n",
    "all_true_labels  = []   \n",
    "all_predictions  = [] \n",
    "all_probs_pos    = []  \n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    fold_dir = f\"training_ViT_fold_{fold}\"\n",
    "    ckpts = [os.path.join(fold_dir, c) for c in os.listdir(fold_dir) if c.startswith(\"checkpoint\")]\n",
    "    if not ckpts:\n",
    "        print(f\" No checkpoint found for fold {fold}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    ckpts.sort(key=lambda p: int(p.split(\"-\")[-1]))\n",
    "    latest_ckpt = ckpts[-1]\n",
    "    print(f\"--- Fold {fold}: loading {latest_ckpt} ---\")\n",
    "\n",
    "    val_df = df_master.iloc[val_idx].reset_index(drop=True)\n",
    "    val_dataset = Dataset.from_pandas(val_df).with_transform(transform)\n",
    "\n",
    "    model = ViTForImageClassification.from_pretrained(latest_ckpt)\n",
    "\n",
    "    eval_args = TrainingArguments(\n",
    "        output_dir=\"tmp_eval\",\n",
    "        per_device_eval_batch_size=16,\n",
    "        remove_unused_columns=False,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    eval_trainer = Trainer(\n",
    "        model=model,\n",
    "        args=eval_args,\n",
    "        data_collator=collate_fn,\n",
    "    )\n",
    "\n",
    "    preds_output = eval_trainer.predict(val_dataset)\n",
    "    logits = preds_output.predictions         \n",
    "    y_true = preds_output.label_ids            \n",
    "    y_pred = np.argmax(logits, axis=1)\n",
    "\n",
    "    logits_shifted = logits - np.max(logits, axis=1, keepdims=True)\n",
    "    exp_logits = np.exp(logits_shifted)\n",
    "    probs = exp_logits / exp_logits.sum(axis=1, keepdims=True)\n",
    "    probs_pos = probs[:, 1]\n",
    "\n",
    "    all_true_labels.append(y_true)\n",
    "    all_predictions.append(y_pred)\n",
    "    all_probs_pos.append(probs_pos)\n",
    "\n",
    "    acc  = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, pos_label=1)\n",
    "    rec  = recall_score(y_true, y_pred, pos_label=1)\n",
    "    f1   = f1_score(y_true, y_pred, pos_label=1)\n",
    "\n",
    "    all_metrics.append({\n",
    "        \"fold\": fold,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "    })\n",
    "\n",
    "all_true_labels = np.concatenate(all_true_labels, axis=0)\n",
    "all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "all_probs_pos   = np.concatenate(all_probs_pos, axis=0)\n",
    "\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "display(metrics_df)\n",
    "\n",
    "print(\"\\n===== Mean ± Std across folds =====\")\n",
    "for col in [\"accuracy\", \"precision\", \"recall\", \"f1\"]:\n",
    "    print(f\"{col:10s}: {metrics_df[col].mean():.4f} ± {metrics_df[col].std():.4f}\")\n",
    "\n",
    "metrics_df.to_csv(\"cv_metrics_summary.csv\", index=False)\n",
    "print(\"\\nSaved to cv_metrics_summary.csv\")\n",
    "\n",
    "cm = confusion_matrix(all_true_labels, all_predictions)\n",
    "acc_micro = (cm.trace()) / cm.sum()\n",
    "print(f\"\\nMicro accuracy (all folds combined): {acc_micro:.4f}\\n\")\n",
    "print(classification_report(all_true_labels, all_predictions, digits=4))\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=[\"Pred 0\", \"Pred 1\"],\n",
    "    yticklabels=[\"True 0\", \"True 1\"],\n",
    ")\n",
    "plt.title(\"Overall Confusion Matrix (all folds)\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(all_true_labels, all_probs_pos)\n",
    "ap = average_precision_score(all_true_labels, all_probs_pos)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(recall, precision, label=f\"All folds (AP = {ap:.3f})\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision–Recall Curve (5-fold combined)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Combined Average Precision: {ap:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDz-k6AFwiFl"
   },
   "source": [
    "# **Train Final Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "id": "w9jC0kNpwkO1",
    "outputId": "632f175d-60e6-48aa-a59c-98345cad05c0"
   },
   "outputs": [],
   "source": [
    "print(\"--- Training Final Model on All Data ---\")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "final_train_df = df_master\n",
    "final_train_dataset = Dataset.from_pandas(final_train_df).with_transform(transform)\n",
    "\n",
    "final_model = ViTForImageClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "final_training_args = TrainingArguments(\n",
    "    output_dir=\"final_vit_model_training\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=15,\n",
    "    logging_steps=10,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.05,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    remove_unused_columns=False,\n",
    "    use_mps_device=torch.backends.mps.is_available(),\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "final_trainer = CustomTrainer(\n",
    "    model=final_model,\n",
    "    args=final_training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=final_train_dataset,\n",
    "    tokenizer=feature_extractor,\n",
    "    class_weights=class_weights_tensor,\n",
    ")\n",
    "\n",
    "final_trainer.train()\n",
    "\n",
    "final_export_dir = \"final_vit_model\"\n",
    "os.makedirs(final_export_dir, exist_ok=True)\n",
    "\n",
    "final_trainer.save_model(final_export_dir)\n",
    "feature_extractor.save_pretrained(final_export_dir)\n",
    "torch.save({\"id2label\": id2label, \"label2id\": label2id},\n",
    "           os.path.join(final_export_dir, \"label_map.pt\"))\n",
    "\n",
    "print(f\"--- Final Model Saved to: {final_export_dir} ---\")\n",
    "\n",
    "shutil.rmtree(\"final_vit_model_training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHpQjtYmx4Iu"
   },
   "source": [
    "# **Try final the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UjRH_Fb1x6uN"
   },
   "outputs": [],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\"final_vit_model\")\n",
    "processor = ViTImageProcessor.from_pretrained(\"final_vit_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sTWa_0aWx9Zi",
    "outputId": "7e99c65e-8c50-4dfe-c1f7-ecc089ac6268"
   },
   "outputs": [],
   "source": [
    "clf = pipeline(\n",
    "    \"image-classification\",\n",
    "    model= \"final_vit_model\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DD0MXE2mx_m7",
    "outputId": "9717b089-1704-48f0-a502-dff0f3135f51"
   },
   "outputs": [],
   "source": [
    "img_path = \"/content/Dataset/1/DM004_M_R.png\"\n",
    "\n",
    "print(clf(img_path, top_k=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMtGB3rJxtG_"
   },
   "source": [
    "# **Save all content to Google Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_9Jvg6DRxsgU",
    "outputId": "a57e80c4-201a-4748-8251-dcd29ee847a2"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, shutil, pathlib\n",
    "\n",
    "SAVE_DIR = \"/content/drive/MyDrive/colab_backups\"\n",
    "SRC_ROOT = \"/content\"\n",
    "\n",
    "folder_prefixes = (\"export_vit_fold_\", \"training_ViT_fold_\")\n",
    "extra_folders   = {\"Dataset\", \"sample_data\", \"final_vit_model\"}   \n",
    "\n",
    "pathlib.Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for name in os.listdir(SRC_ROOT):\n",
    "    src_path = os.path.join(SRC_ROOT, name)\n",
    "    if os.path.isdir(src_path):\n",
    "        if name.startswith(folder_prefixes) or name in extra_folders:\n",
    "            dst_path = os.path.join(SAVE_DIR, name)\n",
    "            print(f\"Copying folder {src_path} -> {dst_path}\")\n",
    "            shutil.copytree(src_path, dst_path, dirs_exist_ok=True)\n",
    "\n",
    "csv_src = os.path.join(SRC_ROOT, \"cv_metrics_summary.csv\")\n",
    "if os.path.exists(csv_src):\n",
    "    csv_dst = os.path.join(SAVE_DIR, \"cv_metrics_summary.csv\")\n",
    "    print(f\"Copying file {csv_src} -> {csv_dst}\")\n",
    "    shutil.copy2(csv_src, csv_dst)\n",
    "else:\n",
    "    print(\"cv_metrics_summary.csv not found in /content\")\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPeSoOn4irOa"
   },
   "source": [
    "# **Grad-Cam**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FOLDER_PATH = \"final_vit_model\"\n",
    "IMAGE_PATH_1 = \"img/[1] DFU/DM115_F_R.png\"  \n",
    "IMAGE_PATH_2 = \"img/[1] DFU/DM115_F_L.png\" \n",
    "TARGET_CLASS_INDEX = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_transform(tensor):\n",
    "    tensor = tensor[:, 1:, :]         \n",
    "\n",
    "    B, N, C = tensor.shape\n",
    "    H = W = int(N ** 0.5)             \n",
    "\n",
    "    tensor = tensor.reshape(B, H, W, C)   \n",
    "    tensor = tensor.permute(0, 3, 1, 2) \n",
    "    return tensor\n",
    "\n",
    "class HuggingfaceToTensorModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(HuggingfaceToTensorModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_available()\n",
    "    else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ViTImageProcessor.from_pretrained(MODEL_FOLDER_PATH)\n",
    "hf_model = ViTForImageClassification.from_pretrained(MODEL_FOLDER_PATH)\n",
    "\n",
    "model = HuggingfaceToTensorModelWrapper(hf_model).to(device).eval()\n",
    "\n",
    "target_layers = [model.model.vit.encoder.layer[-1].layernorm_before]\n",
    "\n",
    "cam = GradCAM(\n",
    "    model=model,\n",
    "    target_layers=target_layers,\n",
    "    reshape_transform=reshape_transform\n",
    ")\n",
    "\n",
    "targets = [ClassifierOutputTarget(TARGET_CLASS_INDEX)]\n",
    "\n",
    "image_paths = [IMAGE_PATH_1, IMAGE_PATH_2]\n",
    "rgb_images = []          \n",
    "cam_visualizations = []  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in image_paths:\n",
    "    pil_img = Image.open(p).convert(\"RGB\")\n",
    "\n",
    "    img_np = np.array(pil_img).astype(np.float32) / 255.0 \n",
    "    rgb_images.append(img_np)\n",
    "\n",
    "    inputs = processor(images=pil_img, return_tensors=\"pt\")\n",
    "    input_tensor = inputs[\"pixel_values\"].to(device)  \n",
    "\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0, :] \n",
    "\n",
    "    H, W, _ = img_np.shape\n",
    "    grayscale_cam_resized = cv2.resize(grayscale_cam, (W, H))\n",
    "\n",
    "    cam_vis = show_cam_on_image(img_np, grayscale_cam_resized, use_rgb=True)\n",
    "    cam_visualizations.append(cam_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(7, 4))\n",
    "\n",
    "fig.suptitle(\n",
    "    f\"Grad-CAM Visualization (Target Class: {TARGET_CLASS_INDEX})\",\n",
    "    fontsize=16,\n",
    "    y=1.03\n",
    ")\n",
    "\n",
    "axes[0].imshow(rgb_images[0])\n",
    "axes[0].set_title(\"Original Right Foot\", fontsize=10, pad=4)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(rgb_images[1])\n",
    "axes[1].set_title(\"Original Left Foot\", fontsize=10, pad=4)\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(cam_visualizations[0])\n",
    "axes[2].set_title(\"Grad-CAM Right Foot\", fontsize=10, pad=4)\n",
    "axes[2].axis('off')\n",
    "\n",
    "axes[3].imshow(cam_visualizations[1])\n",
    "axes[3].set_title(\"Grad-CAM Left Foot\", fontsize=10, pad=4)\n",
    "axes[3].axis('off')\n",
    "\n",
    "fig.subplots_adjust(\n",
    "    wspace=0.05,\n",
    "    left=0.03,\n",
    "    right=0.97,\n",
    "    top=0.85,\n",
    "    bottom=0.05\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "-NOc2Kxys-k1",
    "08IgchiktFty",
    "MRBw-L0EtWfp",
    "uOs1XD3tteF3",
    "tg_d61nLDhSn",
    "qxpgOMh6DuY9",
    "9s48gl1RUUB7",
    "oDz-k6AFwiFl",
    "eHpQjtYmx4Iu",
    "JMtGB3rJxtG_"
   ],
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
